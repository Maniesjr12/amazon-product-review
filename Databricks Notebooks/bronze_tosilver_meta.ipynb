{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02f49edf-75d2-4c2f-9e5e-a3f9135bf173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dbutils.widgets.text(\"categories\", '[\"Books\", \"Electronics\"]', \"Categories to Process\")\n",
    "\n",
    "# Get the parameter value\n",
    "categories_param = dbutils.widgets.get(\"categories\")\n",
    "print(f\" Received parameter from ADF: {categories_param}\")\n",
    "\n",
    "# Parse the JSON string into a Python list\n",
    "import json\n",
    "categories = json.loads(categories_param)\n",
    "print(f\" Categories to process: {categories}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Configure storage\n",
    "storage_account_name = \"your account name\"\n",
    "storage_account_key = \"your storage account key\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "\n",
    "bronze_base_path = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net\"\n",
    "silver_base_path = f\"abfss://silver@{storage_account_name}.dfs.core.windows.net\"\n",
    "\n",
    "# categories = [\"Electronics\", \"Books\"]\n",
    "\n",
    "print(\"Configuration complete!\")\n",
    "\n",
    "\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"main_category\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"average_rating\", FloatType(), True),\n",
    "    StructField(\"rating_number\", IntegerType(), True),\n",
    "    StructField(\"features\", ArrayType(StringType()), True),\n",
    "    StructField(\"description\", ArrayType(StringType()), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"images\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"hi_res\", StringType(), True),\n",
    "            StructField(\"thumb\", StringType(), True),\n",
    "            StructField(\"large\", StringType(), True),\n",
    "            StructField(\"variant\", StringType(), True)\n",
    "        ])\n",
    "    ), True),\n",
    "    StructField(\"videos\", ArrayType(StringType()), True),\n",
    "    StructField(\"store\", StringType(), True),\n",
    "    StructField(\"categories\", ArrayType(StringType()), True),\n",
    "    StructField(\"details\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"parent_asin\", StringType(), True),\n",
    "    StructField(\"bought_together\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "print(\"Metadata schema defined!\")\n",
    "\n",
    "\n",
    "sample_path = f\"{bronze_base_path}/metadata/Electronics\"\n",
    "print(f\" Reading sample from: {sample_path}\")\n",
    "\n",
    "df_meta_sample = spark.read.schema(metadata_schema).json(sample_path, multiLine=False)\n",
    "\n",
    "print(\" Metadata Schema:\")\n",
    "df_meta_sample.printSchema()\n",
    "\n",
    "print(\"\\n Sample Metadata Records:\")\n",
    "df_meta_sample.show(3, truncate=True)\n",
    "\n",
    "print(f\"\\n Total Metadata Records: {df_meta_sample.count():,}\")\n",
    "\n",
    "def clean_metadata(category_name):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" Processing metadata for: {category_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    input_path = f\"{bronze_base_path}/metadata/{category_name}\"\n",
    "    print(f\" Reading from: {input_path}\")\n",
    "    \n",
    "    df_raw = spark.read.schema(metadata_schema).json(input_path, multiLine=False)\n",
    "    raw_count = df_raw.count()\n",
    "    print(f\" Raw metadata records: {raw_count:,}\")\n",
    "    \n",
    "    # 2. CLEAN & TRANSFORM\n",
    "    df_cleaned = df_raw \\\n",
    "        .dropDuplicates([\"parent_asin\"]) \\\n",
    "        .filter(F.col(\"parent_asin\").isNotNull()) \\\n",
    "        .withColumn(\n",
    "            \"features_text\",\n",
    "            F.concat_ws(\" | \", F.col(\"features\"))\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"description_text\",\n",
    "            F.concat_ws(\" \", F.col(\"description\"))\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"has_price\",\n",
    "            F.when(F.col(\"price\").isNotNull(), True).otherwise(False)\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"price_cleaned\",\n",
    "            F.when(F.col(\"price\").isNull(), 0.0)\n",
    "            .when(F.col(\"price\") <= 0, 0.0)\n",
    "            .otherwise(F.col(\"price\"))\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"image_count\",\n",
    "            F.size(F.col(\"images\"))\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"video_count\",\n",
    "            F.size(F.col(\"videos\"))\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"category\",\n",
    "            F.lit(category_name)\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"processing_timestamp\",\n",
    "            F.current_timestamp()\n",
    "        )\n",
    "    \n",
    "    df_cleaned = df_cleaned \\\n",
    "        .withColumn(\n",
    "            \"product_details_json\",\n",
    "            F.to_json(F.col(\"details\"))\n",
    "        )\n",
    "    \n",
    "    df_cleaned = df_cleaned \\\n",
    "        .withColumn(\"brand\", F.col(\"details\").getItem(\"Brand\")) \\\n",
    "        .withColumn(\"color\", F.col(\"details\").getItem(\"Color\")) \\\n",
    "        .withColumn(\"size_info\", F.col(\"details\").getItem(\"Size\")) \\\n",
    "        .withColumn(\"material\", F.col(\"details\").getItem(\"Material\"))\n",
    "    \n",
    "    cleaned_count = df_cleaned.count()\n",
    "    print(f\" Cleaned records: {cleaned_count:,}\")\n",
    "    print(f\"Removed records: {raw_count - cleaned_count:,}\")\n",
    "    \n",
    "    # 4. SELECT final columns\n",
    "    df_final = df_cleaned.select(\n",
    "        \"parent_asin\",\n",
    "        \"main_category\",\n",
    "        \"title\",\n",
    "        \"average_rating\",\n",
    "        \"rating_number\",\n",
    "        \"features_text\",\n",
    "        \"description_text\",\n",
    "        \"price_cleaned\",\n",
    "        \"store\",\n",
    "        \"brand\",\n",
    "        \"color\",\n",
    "        \"size_info\",\n",
    "        \"material\",\n",
    "        \"product_details_json\",  # Full details as JSON\n",
    "        \"image_count\",\n",
    "        \"video_count\",\n",
    "        \"category\",\n",
    "        \"processing_timestamp\"\n",
    "    )\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CELL 5: Process All Categories\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "df_all_metadata = None\n",
    "\n",
    "for category in categories:\n",
    "    df_category = clean_metadata(category)\n",
    "    \n",
    "    if df_all_metadata is None:\n",
    "        df_all_metadata = df_category\n",
    "    else:\n",
    "        df_all_metadata = df_all_metadata.union(df_category)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\" TOTAL CLEANED METADATA: {df_all_metadata.count():,}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\n Sample Cleaned Metadata:\")\n",
    "df_all_metadata.show(5, truncate=True)\n",
    "\n",
    "print(\"\\n Metadata by Category:\")\n",
    "df_all_metadata.groupBy(\"category\").count().orderBy(F.desc(\"count\")).show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CELL 6: Write to Silver Layer\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "output_path = f\"{silver_base_path}/metadata_cleaned\"\n",
    "\n",
    "print(f\"\\n Writing to Silver layer: {output_path}\")\n",
    "\n",
    "df_all_metadata.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .parquet(output_path)\n",
    "\n",
    "print(\" Silver layer write complete!\")\n",
    "\n",
    "# CELL 7: Verify\n",
    "\n",
    "df_silver_verify = spark.read.parquet(output_path)\n",
    "\n",
    "print(\"\\nVerification:\")\n",
    "print(\"Check for cleaned price nulls:\")\n",
    "\n",
    "null = df_silver_verify.filter(F.col(\"price_cleaned\").isNull()).count()\n",
    "print(f\"Null price records: {null:,}\")\n",
    "\n",
    "print(f\" Total metadata records: {df_silver_verify.count():,}\")\n",
    "\n",
    "print(\"\\n Data Quality Metrics:\")\n",
    "df_silver_verify.select(\n",
    "    F.count(\"*\").alias(\"total_products\"),\n",
    "    F.avg(\"price_cleaned\").alias(\"avg_price\"),\n",
    "    F.avg(\"average_rating\").alias(\"avg_rating\"),\n",
    "    F.sum(F.when(F.col(\"store\").isNotNull(), 1).otherwise(0)).alias(\"products_with_store\")\n",
    ").show()\n",
    "\n",
    "print(\"\\n Notebook 2 Complete! Metadata is now in Silver layer.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_tosilver_meta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
