{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29bae4f6-a32c-47b5-aeae-0e59e1fc069b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-4699401764271053>, line 7\u001b[0m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Received parameter from ADF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategories_param\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
       "\u001b[0;32m----> 7\u001b[0m categories \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(categories_param)\n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Categories to process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategories\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F  \n",
       "\n",
       "File \u001b[0;32m/usr/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n",
       "\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
       "\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
       "\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n",
       "\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
       "\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n",
       "\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
       "\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
       "\n",
       "File \u001b[0;32m/usr/lib/python3.12/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n",
       "\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
       "\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
       "\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n",
       "\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
       "\n",
       "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 8 (char 7)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {
        "categories": {
         "defaultValue": "[\"Books\", \"Electronics\"]",
         "label": "Categories to Process",
         "name": "categories",
         "options": {
          "autoCreated": null,
          "validationRegex": null,
          "widgetType": "text"
         },
         "widgetType": "text"
        }
       },
       "arguments": {
        "categories": "\"Books\",\"Electronics\""
       },
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "JSONDecodeError",
        "evalue": "Extra data: line 1 column 8 (char 7)"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
        "File \u001b[0;32m<command-4699401764271053>, line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Received parameter from ADF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategories_param\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m categories \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(categories_param)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Categories to process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategories\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F  \n",
        "File \u001b[0;32m/usr/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
        "File \u001b[0;32m/usr/lib/python3.12/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
        "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 8 (char 7)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dbutils.widgets.text(\"categories\", '[\"Books\", \"Electronics\"]', \"Categories to Process\")\n",
    "\n",
    "categories_param = dbutils.widgets.get(\"categories\")\n",
    "print(f\" Received parameter from ADF: {categories_param}\")\n",
    "\n",
    "import json\n",
    "categories = json.loads(categories_param)\n",
    "print(f\" Categories to process: {categories}\")\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F  \n",
    "from pyspark.sql.types import *         \n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "storage_account_name = \"your account name\"\n",
    "storage_account_key = \"your storage account key\"\n",
    "\n",
    "# Set Spark configuration to access Azure Data Lake\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "\n",
    "bronze_base_path = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net\"\n",
    "silver_base_path = f\"abfss://silver@{storage_account_name}.dfs.core.windows.net\"\n",
    "\n",
    "# Define categories we're processing\n",
    "#categories = [\"Electronics\", \"Books\"]\n",
    "\n",
    "print(\" Configuration complete!\")\n",
    "print(f\" Bronze path: {bronze_base_path}\")\n",
    "print(f\" Silver path: {silver_base_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample_path = f\"{bronze_base_path}/reviews/*\"\n",
    "\n",
    "# Read JSON with schema inference (Spark figures out the structure)\n",
    "df_sample = spark.read.json(sample_path, multiLine=False)\n",
    "\n",
    "print(\" Raw Data Schema:\")\n",
    "df_sample.printSchema()\n",
    "\n",
    "print(\"\\n Sample Records (first 5 rows):\")\n",
    "df_sample.show(5, truncate=False) \n",
    "\n",
    "print(f\"\\n Total Records in : {df_sample.count():,}\")\n",
    "\n",
    "# Check for null values in key fields\n",
    "print(\"\\n Null Value Counts:\")\n",
    "df_sample.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c) \n",
    "    for c in df_sample.columns\n",
    "]).show()\n",
    "\n",
    "\n",
    "# 3: Define Explicit Schema (Best Practice)\n",
    "\n",
    "\n",
    "\n",
    "# Define schema for reviews based on your JSON structure\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"rating\", FloatType(), True),           # Star rating (1-5)\n",
    "    StructField(\"title\", StringType(), True),            # Review title\n",
    "    StructField(\"text\", StringType(), True),             # Review content\n",
    "    StructField(\"images\", ArrayType(                     # Array of image objects\n",
    "        StructType([\n",
    "            StructField(\"small_image_url\", StringType(), True),\n",
    "            StructField(\"medium_image_url\", StringType(), True),\n",
    "            StructField(\"large_image_url\", StringType(), True),\n",
    "            StructField(\"attachment_type\", StringType(), True)\n",
    "        ])\n",
    "    ), True),\n",
    "    StructField(\"asin\", StringType(), True),             # Product ASIN (not used for joins)\n",
    "    StructField(\"parent_asin\", StringType(), True),      # Parent ASIN (JOIN KEY!)\n",
    "    StructField(\"user_id\", StringType(), True),          # User identifier\n",
    "    StructField(\"timestamp\", LongType(), True),          # Unix timestamp (milliseconds)\n",
    "    StructField(\"helpful_vote\", IntegerType(), True),    # Number of helpful votes\n",
    "    StructField(\"verified_purchase\", BooleanType(), True) # Verified purchase flag\n",
    "])\n",
    "\n",
    "print(\" Schema defined!\")\n",
    "\n",
    "\n",
    "# 4: Read & Clean Reviews Data\n",
    "\n",
    "\n",
    "# Function to process reviews for a single category\n",
    "def clean_reviews(category_name):\n",
    "   \n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" Processing category: {category_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 1. READ: Load raw data from Bronze\n",
    "    input_path = f\"{bronze_base_path}/reviews/{category_name}\"\n",
    "    \n",
    "    print(f\" Reading from: {input_path}\")\n",
    "    \n",
    "    df_raw = spark.read.schema(reviews_schema).json(input_path, multiLine=False)\n",
    "    \n",
    "    raw_count = df_raw.count()\n",
    "    print(f\" Raw records: {raw_count:,}\")\n",
    "    \n",
    "    # 2. CLEAN: Data quality transformations\n",
    "    df_cleaned = df_raw \\\n",
    "        .dropDuplicates([\"user_id\", \"parent_asin\", \"timestamp\"]) \\\n",
    "        .filter(F.col(\"parent_asin\").isNotNull()) \\\n",
    "        .filter(F.col(\"rating\").isNotNull()) \\\n",
    "        .filter(F.col(\"rating\").between(1, 5)) \\\n",
    "        .withColumn(\n",
    "            \"review_date\",\n",
    "            F.to_timestamp(F.col(\"timestamp\") / 1000)  # Convert Unix ms to timestamp\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"review_text_length\",\n",
    "            F.length(F.col(\"text\"))  # Calculate review length\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"has_images\",\n",
    "            F.when(F.size(F.col(\"images\")) > 0, True).otherwise(False)  # Boolean flag\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"image_count\",\n",
    "            F.size(F.col(\"images\"))  # Count of images\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"category\",\n",
    "            F.lit(category_name)  \n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"processing_timestamp\",\n",
    "            F.current_timestamp()  # Audit column: when was this processed?\n",
    "        )\n",
    "    \n",
    "    cleaned_count = df_cleaned.count()\n",
    "    print(f\" Cleaned records: {cleaned_count:,}\")\n",
    "    print(f\"  Removed records: {raw_count - cleaned_count:,} ({((raw_count - cleaned_count) / raw_count * 100):.2f}%)\")\n",
    "    \n",
    "    # 3. SELECT: Final columns for Silver layer (drop raw nested fields for now)\n",
    "    df_final = df_cleaned.select(\n",
    "        \"parent_asin\",            # JOIN KEY\n",
    "        \"user_id\",                # User identifier\n",
    "        \"rating\",                 # Star rating\n",
    "        \"title\",                  # Review title\n",
    "        \"text\",                   # Review content\n",
    "        \"review_text_length\",     # Derived: length of review\n",
    "        \"timestamp\",              # Original Unix timestamp\n",
    "        \"review_date\",            # Converted timestamp\n",
    "        \"helpful_vote\",           # Helpfulness votes\n",
    "        \"verified_purchase\",      # Verified flag\n",
    "        \"has_images\",             # Derived: boolean\n",
    "        \"image_count\",            # Derived: count\n",
    "        \"category\",               # Category name\n",
    "        \"processing_timestamp\"    # Audit column\n",
    "    )\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "df_all_reviews = None\n",
    "\n",
    "for category in categories:\n",
    "    df_category = clean_reviews(category)\n",
    "    \n",
    "    if df_all_reviews is None:\n",
    "        df_all_reviews = df_category\n",
    "    else:\n",
    "        df_all_reviews = df_all_reviews.union(df_category)  # Combine datasets\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\" TOTAL CLEANED REVIEWS: {df_all_reviews.count():,}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Display sample of cleaned data\n",
    "print(\"\\n Sample Cleaned Data:\")\n",
    "df_all_reviews.show(10, truncate=True)\n",
    "\n",
    "# Show data distribution by category\n",
    "print(\"\\n Records by Category:\")\n",
    "df_all_reviews.groupBy(\"category\").count().orderBy(F.desc(\"count\")).show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Output path for Silver reviews\n",
    "output_path = f\"{silver_base_path}/reviews_cleaned\"\n",
    "\n",
    "print(f\"\\n Writing to Silver layer: {output_path}\")\n",
    "\n",
    "\n",
    "df_all_reviews.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .parquet(output_path)\n",
    "\n",
    "print(\" Silver layer write complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_silver_verify = spark.read.parquet(output_path)\n",
    "\n",
    "print(\"\\n Verification:\")\n",
    "print(f\" Total records in Silver: {df_silver_verify.count():,}\")\n",
    "print(f\" Schema:\")\n",
    "df_silver_verify.printSchema()\n",
    "\n",
    "\n",
    "print(\"\\n Data Quality Metrics:\")\n",
    "df_silver_verify.select(\n",
    "    F.count(\"*\").alias(\"total_records\"),\n",
    "    F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "    F.avg(\"helpful_vote\").alias(\"avg_helpful_votes\"),\n",
    "    F.sum(F.when(F.col(\"verified_purchase\") == True, 1).otherwise(0)).alias(\"verified_purchases\"),\n",
    "    F.sum(F.when(F.col(\"has_images\") == True, 1).otherwise(0)).alias(\"reviews_with_images\")\n",
    ").show()\n",
    "\n",
    "print(\"\\n Notebook 1 Complete! Reviews are now in Silver layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc34ce62-3409-4880-8fdd-01bc4386bc05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_to_sliver_reviews",
   "widgets": {
    "categories": {
     "currentValue": "\"Books\",\"Electronics\"",
     "nuid": "286ebb12-4d68-4ff8-892e-48c499184dd4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "[\"Books\", \"Electronics\"]",
      "label": "Categories to Process",
      "name": "categories",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "[\"Books\", \"Electronics\"]",
      "label": "Categories to Process",
      "name": "categories",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
